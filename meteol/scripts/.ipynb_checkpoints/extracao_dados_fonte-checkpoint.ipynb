{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee28da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55a58423",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'required' is an invalid argument for positionals",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA EM FORMATO INVALIDO! \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(s)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m argparse\u001b[38;5;241m.\u001b[39mArgumentTypeError(msg)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_argument\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_processamento\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhelp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFORMATO ACEITAVEL: YYYY-MM-DD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_date\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/argparse.py:1404\u001b[0m, in \u001b[0;36m_ActionsContainer.add_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdest\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdest supplied twice for positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1404\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_positional_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;66;03m# otherwise, we're adding an optional argument\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_kwargs(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/argparse.py:1520\u001b[0m, in \u001b[0;36m_ActionsContainer._get_positional_kwargs\u001b[0;34m(self, dest, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1519\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is an invalid argument for positionals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# mark positional arguments as required if at least one is\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# always required\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnargs\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [OPTIONAL, ZERO_OR_MORE]:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'required' is an invalid argument for positionals"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a8b0ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hdfs(file_name, fonte):\n",
    "    \n",
    "    spark = SparkSession.builder.appName('Get HDFS File').getOrCreate()\n",
    "  \n",
    "    df = spark.read.csv(\"/hdfs/data/order/{}/input/{}_{}.csv\".format(file_name, fonte, file_name), header=True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45935496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_file_hdfs(file_name, fonte, df):\n",
    "    spark = SparkSession.builder.appName('Backup HDFS File').getOrCreate()\n",
    "  \n",
    "    df.coalesce(1).write.mode('overwrite').option('header','true').csv('/hdfs/data/order/{}/bac/{}_{}.csv'.format(table_name, fonte, table_name))\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65ad07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+--------+\n",
      "|order_id|product_id|unit_price|quantity|discount|\n",
      "+--------+----------+----------+--------+--------+\n",
      "|   10248|        11|        14|      12|       0|\n",
      "|   10248|        42|       9.8|      10|       0|\n",
      "|   10248|        72|      34.8|       5|       0|\n",
      "|   10249|        14|      18.6|       9|       0|\n",
      "|   10249|        51|      42.4|      40|       0|\n",
      "|   10250|        41|       7.7|      10|       0|\n",
      "|   10250|        51|      42.4|      35|    0.15|\n",
      "|   10250|        65|      16.8|      15|    0.15|\n",
      "|   10251|        22|      16.8|       6|    0.05|\n",
      "|   10251|        57|      15.6|      15|    0.05|\n",
      "|   10251|        65|      16.8|      20|       0|\n",
      "|   10252|        20|      64.8|      40|    0.05|\n",
      "|   10252|        33|         2|      25|    0.05|\n",
      "|   10252|        60|      27.2|      40|       0|\n",
      "|   10253|        31|        10|      20|       0|\n",
      "|   10253|        39|      14.4|      42|       0|\n",
      "|   10253|        49|        16|      40|       0|\n",
      "|   10254|        24|       3.6|      15|    0.15|\n",
      "|   10254|        55|      19.2|      21|    0.15|\n",
      "|   10254|        74|         8|      21|       0|\n",
      "+--------+----------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fonte = 'csv'\n",
    "table_name = ''\n",
    "file_name = 'order_details'\n",
    "\n",
    "df = get_file_hdfs(file_name, fonte)\n",
    "  \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "930e75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+--------+\n",
      "|order_id|product_id|unit_price|quantity|discount|\n",
      "+--------+----------+----------+--------+--------+\n",
      "|   10248|        11|        14|      12|       0|\n",
      "|   10248|        42|       9.8|      10|       0|\n",
      "|   10248|        72|      34.8|       5|       0|\n",
      "|   10249|        14|      18.6|       9|       0|\n",
      "|   10249|        51|      42.4|      40|       0|\n",
      "|   10250|        41|       7.7|      10|       0|\n",
      "|   10250|        51|      42.4|      35|    0.15|\n",
      "|   10250|        65|      16.8|      15|    0.15|\n",
      "|   10251|        22|      16.8|       6|    0.05|\n",
      "|   10251|        57|      15.6|      15|    0.05|\n",
      "|   10251|        65|      16.8|      20|       0|\n",
      "|   10252|        20|      64.8|      40|    0.05|\n",
      "|   10252|        33|         2|      25|    0.05|\n",
      "|   10252|        60|      27.2|      40|       0|\n",
      "|   10253|        31|        10|      20|       0|\n",
      "|   10253|        39|      14.4|      42|       0|\n",
      "|   10253|        49|        16|      40|       0|\n",
      "|   10254|        24|       3.6|      15|    0.15|\n",
      "|   10254|        55|      19.2|      21|    0.15|\n",
      "|   10254|        74|         8|      21|       0|\n",
      "+--------+----------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fonte = 'csv'\n",
    "table_name = ''\n",
    "file_name = 'order_details'\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    print(\"OBTENDO ARQUIVO HDFS PARA DATA: {}\".format(args.data_processamento))\n",
    "    df = get_file_hdfs(file_name, fonte)\n",
    "    df.show()\n",
    "    \n",
    "    #Realiza o backup do arquivo no folder backup no hdfs pois existe o script que faz a limpeza dos arquivos do diretorio input  \n",
    "    print(\"GERANDO BACKUP DO ARQUIVO: {}.csv\".format(file_name))\n",
    "    backup_file_hdfs(file_name, fonte, df)\n",
    "    \n",
    "        \n",
    "except Exeption as e:\n",
    "    print(\"ERRO:\", e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dda68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a12ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
